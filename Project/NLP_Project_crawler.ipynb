{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLppTG4cPucK",
        "outputId": "c7b49ee5-7e85-4c51-8538-820fa3eaa4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLsu3z5qQAHj",
        "outputId": "114512a8-71ee-4bd7-b8f1-776d9ea16a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pymorphy2\n",
        "import nltk\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CufHWoLRQYh1"
      },
      "source": [
        "def collect_text(url):\n",
        "  html = urlopen(url).read()\n",
        "  soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "  # kill all script and style elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "  # get text\n",
        "  text = soup.get_text()\n",
        "\n",
        "  # break into lines and remove leading and trailing space on each\n",
        "  lines = (line.strip() for line in text.splitlines())\n",
        "  # break multi-headlines into a line each\n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "  # drop blank lines\n",
        "  def is_date(o):\n",
        "    x = o.split()\n",
        "    if len(x) < 2:\n",
        "      return False\n",
        "    x = x[-2]\n",
        "    if(len(x) == 4 and x[0] == '1' and x[1] == '9'):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  source = ''\n",
        "\n",
        "  kill = False\n",
        "\n",
        "  def TCHEKHOV(chunk):\n",
        "    if 'ЧЕХОВ' in chunk:\n",
        "      return True\n",
        "    return False\n",
        "  \n",
        "  tch = ''\n",
        "\n",
        "  for chunk in (chunks):\n",
        "    if TCHEKHOV(chunk):\n",
        "      tch = chunk\n",
        "    if kill and chunk:\n",
        "      source += chunk\n",
        "      chunk = None\n",
        "      break\n",
        "    if is_date(chunk):\n",
        "      source += (chunk)\n",
        "      source += ' '\n",
        "      chunk = None\n",
        "      kill = True\n",
        "    else:\n",
        "      chunk = None\n",
        "\n",
        "\n",
        "  erase = False\n",
        "\n",
        "  def eraser(chunk):\n",
        "    nonlocal erase\n",
        "    if chunk == 'ПОИСК:':\n",
        "      erase = True\n",
        "    if erase:\n",
        "      return False\n",
        "    if not chunk:\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "  text = '\\n'.join(chunk for chunk in chunks if eraser(chunk))\n",
        "\n",
        "  sentences = nltk.tokenize.sent_tokenize(text)\n",
        "  source = \"Письмо \" + tch + ' ' + source\n",
        "  return source, sentences"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrn9HBO4RL8d"
      },
      "source": [
        "def annotate_text(source, sentences):\n",
        "  end_s = []\n",
        "  for s in sentences:\n",
        "    original = s\n",
        "    original = original.lower()\n",
        "    p = ',:@.-()/!'\n",
        "    original_cleaned = original\n",
        "    for i in p:\n",
        "      original_cleaned = original_cleaned.replace(i, '')\n",
        "    original_splitted = [ i for i in re.split(r'\\W+',original) if i]\n",
        "    \n",
        "    lemmatized = []\n",
        "    for j in original_splitted:\n",
        "        lemmatized.append(morph.parse(j)[0].normal_form)\n",
        "    tags = []\n",
        "    for word in original_splitted:   \n",
        "        tag = morph.parse(word)[0].tag\n",
        "        tags.append(str(tag))\n",
        "    end_s.append((source, s, original_splitted, lemmatized, tags))\n",
        "  return end_s"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR_-7_F5R3am"
      },
      "source": [
        "corpora = []\n",
        "for i in range(24, 354):\n",
        "  if i == 193 or i == 194 or i == 202 or i == 203 or i == 204 or i == 205 or i == 313 or i == 326:\n",
        "    continue\n",
        "  if i < 100:\n",
        "    url = 'http://apchekhov.ru/books/item/f00/s00/z0000020/st0' + str(i) + '.shtml'\n",
        "  else:\n",
        "    url = 'http://apchekhov.ru/books/item/f00/s00/z0000020/st' + str(i) + '.shtml'\n",
        "  source, sentences = collect_text(url)\n",
        "  annotated_text = annotate_text(source, sentences)\n",
        "  corpora += (annotated_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30V1dhiuQAsS"
      },
      "source": [
        "import json\n",
        "\n",
        "data = corpora\n",
        "\n",
        "with open('corpORAORA.txt', 'w') as outfile:\n",
        "    json.dump(data, outfile)\n",
        "\n",
        "with open('corpORAORA.txt') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(data[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}